

## ğŸ§­ 1. Overview and Objectives

The presentation introduces **Natural Language Processing (NLP)** â€” a field of AI that enables machines to understand, interpret, and generate human language.
It explores:

* Common NLP tasks
* Word embedding techniques
* Traditional models (N-gram, LSA, Random Indexing)
* Neural network models (Word2Vec)
* Evaluation of word embeddings
* Practical applications and comparisons

---

## ğŸ—£ï¸ 2. Why NLP?

Language is described as the first great product of human cognition.
It allows:

* Collaboration and communication
* Expression of emotions, thoughts, and decisions
* Information storage through narratives

Thus, NLP is the bridge enabling **machines to communicate with humans**.

---

## ğŸ“‹ 3. Common NLP Tasks

1. **Text Classification** â€“ Assigning texts to predefined categories.

   * Examples:

     * *Genre classification* (news, legal, scientific)
     * *Language identification*
     * *Spam detection*
2. **Information Extraction** â€“ Identifying entities and relationships.
3. **Machine Reading/Translation** â€“ Automatic understanding and translation of text.

All these require **understanding meaning and semantics**.

---

## ğŸ” 4. Meaning and Semantics

* NLP systems need to capture *semantics* â€” the meaning of words and sentences.
* Challenges: how to **represent meaning numerically**.
* The document mentions Ellie Pavlickâ€™s work on **symbols and grounding** in large language models (LLMs), emphasizing constructing meaning from experience.

---

## ğŸ”¢ 5. Simple Models: N-Gram Language Models

* A **language model** estimates the probability of a sequence of words.
* An **n-gram** is a contiguous sequence of n items (letters, syllables, or words).
* Used for:

  * Language identification
  * Text prediction
  * Statistical modeling of text patterns

While simple, n-gram models are limited in capturing long-range dependencies.

---

## ğŸ§© 6. Word Embedding

Word embeddings represent words as **dense vectors** in continuous space.
These vectors encode **semantic relationships** between words:

* Similar words (e.g., â€œkingâ€ and â€œqueenâ€) have similar vector representations.
* Relationships like *â€œMadrid â€“ Spain + France â‰ˆ Parisâ€* emerge.

Embeddings thus bridge discrete language with continuous mathematics.

### Key Ideas:

* Context defines meaning (â€œyou shall know a word by the company it keepsâ€).
* Each word is mapped to a vector learned from co-occurrence in large corpora.
* Distances between vectors approximate **semantic similarity**.

---

## âš–ï¸ 7. Evaluation Methods for Word Embeddings

How to judge embedding quality:

### **Extrinsic Evaluation**

* Embeddings are used in downstream tasks.

  * Examples: POS tagging, Named Entity Recognition (NER), sentiment analysis.
* Measures **task performance** improvements.

### **Intrinsic Evaluation**

* Directly evaluates embeddingsâ€™ linguistic structure.

  * **Relatedness**: Compare cosine similarity vs. human judgments.
  * **Analogy**: Solve problems like *man:king :: woman:queen*.
  * **Categorization**: Cluster words and check category purity.
  * **Selectional preference**: Evaluate how typical a word is for a verb (e.g., *people eat apples*, not *apples eat people*).
  * **Coherence**: Check whether nearby words in embedding space are semantically related.

---

## ğŸ§± 8. Word Embedding Models â€” Taxonomy

Two major approaches:

### 1. **Connectionist (Neural Networkâ€“based) Models**

* Represent knowledge as **weighted connections** between neurons.
* Example: **Word2Vec**

  * **Skip-Gram model**: Predicts context words given a target word.
  * **CBOW (Continuous Bag of Words)**: Predicts the target word given context words.
* Captures linguistic regularities through vector arithmetic.

### 2. **Distributional (Corpus-based) Models**

* Based on co-occurrence patterns in text.
* Famous principle: â€œYou shall know a word by the company it keeps.â€
* Examples: **Latent Semantic Analysis (LSA)** and **Random Indexing (RI)**.

---

## ğŸ§® 9. Latent Semantic Analysis (LSA)

* Developed by **Landauer & Dumais (1997)**.
* Starts with a **term-document frequency matrix** (rows = words, columns = documents).
* Applies **Singular Value Decomposition (SVD)** to reduce dimensionality (usually to ~300).
* Captures higher-order semantic similarities (e.g., *boat* and *ship* become closer).
* Achieved ~51% on TOEFL synonym test â€” comparable to human-level vocabulary knowledge.

---

## ğŸ² 10. Random Indexing (RI)

* Alternative to LSA â€” **computationally simpler** (no SVD).
* Assigns random sparse vectors to documents.
* Wordsâ€™ vectors are updated by summing the vectors of documents they appear in.
* Despite randomness, RI preserves semantic similarity remarkably well.
* TOEFL score ~52%.

### Improvement: **Random Indexing with Permutations (RP)**

* Uses *word windows* instead of documents as context.
* Adds permutations to encode **word order**.
* Achieved ~78% on TOEFL â€” approaching Word2Vecâ€™s performance.

---

## ğŸ§  11. Neural Models: Word2Vec

* Developed by **Mikolov et al. (2013)**.
* Learns embeddings by predicting context (Skip-gram or CBOW).
* Produces powerful, generalizable semantic representations.
* Example:

  * vec(â€œMadridâ€) - vec(â€œSpainâ€) + vec(â€œFranceâ€) â‰ˆ vec(â€œParisâ€).

These embeddings form the foundation for modern **transformer-based LLMs**.

---

## ğŸ§ª 12. Practical Assignment

The document concludes with a hands-on exercise:

* Use **Word2Vec** and **Random Indexing** on large text corpora.
* Perform **TOEFL synonym tasks**.
* Compare performance, accuracy, and speed between the two methods.

---

## ğŸ“š 13. References

The lecture cites major works in NLP and semantics:

* Russell & Norvig â€“ *AI: A Modern Approach*
* Landauer & Dumais â€“ *Latent Semantic Analysis*
* Mikolov et al. â€“ *Word2Vec* papers
* Sahlgren et al. â€“ *Encoding word order with permutations*
* Schnabel et al. â€“ *Evaluation methods for word embeddings*

---

## ğŸ§¾ Summary of Key Concepts

| Concept        | Description                                                    | Example                        |
| -------------- | -------------------------------------------------------------- | ------------------------------ |
| **N-gram**     | Simple probabilistic model based on sequences                  | â€œto be or not to beâ€           |
| **LSA**        | Matrix decomposition to capture latent semantics               | â€œboatâ€ and â€œshipâ€ become close |
| **RI**         | Incremental random vector method                               | Faster but approximate         |
| **Word2Vec**   | Neural model learning semantic relationships                   | â€œking - man + woman â‰ˆ queenâ€   |
| **Evaluation** | Intrinsic (similarity, analogy) / Extrinsic (task performance) | TOEFL synonym test             |

---
