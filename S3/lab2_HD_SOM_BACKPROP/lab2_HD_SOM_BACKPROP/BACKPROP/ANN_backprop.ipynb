{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions of non-linear activations\n",
    "def f_sigmoid(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    else:\n",
    "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
    "\n",
    "\n",
    "def f_softmax(X):\n",
    "    Z = np.sum(np.exp(X), axis=1)\n",
    "    Z = Z.reshape(Z.shape[0], 1)\n",
    "    return np.exp(X) / Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def exit_with_err(err_str):\n",
    "    print(err_str, file=sys.stderr)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functionality of a single hidden layer\n",
    "class Layer:\n",
    "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
    "                 activation=f_sigmoid):\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # Z is the matrix that holds output values\n",
    "        self.Z = np.zeros((batch_size, size[0]))\n",
    "        # The activation function is an externally defined function (with a\n",
    "        # derivative) that is stored here\n",
    "        self.activation = activation\n",
    "\n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        # S is the matrix that holds the inputs to this layer\n",
    "        self.S = None\n",
    "        # D is the matrix that holds the deltas for this layer\n",
    "        self.D = None\n",
    "        # Fp is the matrix that holds the derivatives of the activation function\n",
    "        self.Fp = None\n",
    "\n",
    "        if not is_input:\n",
    "            self.S = np.zeros((batch_size, size[0]))\n",
    "            self.D = np.zeros((batch_size, size[0]))\n",
    "\n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "\n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], batch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layer_config, batch_size=100):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = batch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         activation=f_sigmoid))\n",
    "\n",
    "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
    "        self.layers.append(Layer([layer_config[-1], None],\n",
    "                                 batch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=f_softmax))\n",
    "        print (\"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        \n",
    "        #exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
    "        #This line computes the error at the output layer by subtracting the true labels from the predicted outputs.\n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "            \n",
    "            #exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
    "            #This loop iterates backward through the hidden layers of the network, calculating the delta values \n",
    "            # for each layer based on the deltas of the next layer and the weights connecting them.\n",
    "            \n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=70, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "            \n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                \n",
    "                #exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
    "                # This line updates the weights of the network using the computed deltas and the learning rate (eta). \n",
    "                # Eta is a hyperparameter that controls how much to change the weights during each update.\n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                accuracy_train = 0.0\n",
    "        \n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    accuracy_train += np.sum(yhat == np.argmax(b_labels, axis=1))\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Training error: {1:.5f} and Training accuracy: {2:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train,\n",
    "                                                           accuracy_train/N_train))\n",
    "                \n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                accuracy_test = 0.0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    accuracy_test += np.sum(yhat == np.argmax(b_labels, axis=1))\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Test error: {1:.5f} and Test accuracy: {2:.5f}\").format(out_str,\n",
    "                                                       float(errs)/N_test,\n",
    "                                                       accuracy_test/N_test)\n",
    "\n",
    "            print (out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_bit_vector(labels, nbits):\n",
    "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
    "    for i in range(labels.shape[0]):\n",
    "        bit_vector[i, labels[i]] = 1.0\n",
    "\n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
    "    N = data.shape[0]\n",
    "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
    "\n",
    "    if N % batch_size != 0:\n",
    "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
    "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
    "    chunked_data = []\n",
    "    chunked_labels = []\n",
    "    idx = 0\n",
    "    while idx + batch_size <= N:\n",
    "        chunked_data.append(data[idx:idx+batch_size, :])\n",
    "        if not create_bit_vector:\n",
    "            chunked_labels.append(labels[idx:idx+batch_size])\n",
    "        else:\n",
    "            bit_vector = label_to_bit_vector(labels[idx:idx+batch_size], 10)\n",
    "            chunked_labels.append(bit_vector)\n",
    "\n",
    "        idx += batch_size\n",
    "\n",
    "    return chunked_data, chunked_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
    "    \n",
    "    print (\"Creating data...\")\n",
    "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    print (\"Done!\")\n",
    "\n",
    "\n",
    "    return batched_train_data, batched_train_labels,  batched_valid_data, batched_valid_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 11:15:49.641124: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
    "\n",
    "Xtr = Xtr.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "Xtr = Xtr.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "Xtr /= 255\n",
    "X_test /= 255\n",
    "print(Xtr.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.44910 and Training accuracy: 0.55090 Test error: 0.44660 and Test accuracy: 0.55340\n",
      "[   1]  Training error: 0.07512 and Training accuracy: 0.92488 Test error: 0.07370 and Test accuracy: 0.92630\n",
      "[   2]  Training error: 0.05305 and Training accuracy: 0.94695 Test error: 0.05800 and Test accuracy: 0.94200\n",
      "[   3]  Training error: 0.03835 and Training accuracy: 0.96165 Test error: 0.04390 and Test accuracy: 0.95610\n",
      "[   4]  Training error: 0.03050 and Training accuracy: 0.96950 Test error: 0.03790 and Test accuracy: 0.96210\n",
      "[   5]  Training error: 0.02765 and Training accuracy: 0.97235 Test error: 0.03600 and Test accuracy: 0.96400\n",
      "[   6]  Training error: 0.02750 and Training accuracy: 0.97250 Test error: 0.03690 and Test accuracy: 0.96310\n",
      "[   7]  Training error: 0.02073 and Training accuracy: 0.97927 Test error: 0.03480 and Test accuracy: 0.96520\n",
      "[   8]  Training error: 0.02107 and Training accuracy: 0.97893 Test error: 0.03460 and Test accuracy: 0.96540\n",
      "[   9]  Training error: 0.01738 and Training accuracy: 0.98262 Test error: 0.03310 and Test accuracy: 0.96690\n",
      "[  10]  Training error: 0.01902 and Training accuracy: 0.98098 Test error: 0.03400 and Test accuracy: 0.96600\n",
      "[  11]  Training error: 0.02207 and Training accuracy: 0.97793 Test error: 0.03910 and Test accuracy: 0.96090\n",
      "[  12]  Training error: 0.02042 and Training accuracy: 0.97958 Test error: 0.03640 and Test accuracy: 0.96360\n",
      "[  13]  Training error: 0.01590 and Training accuracy: 0.98410 Test error: 0.03590 and Test accuracy: 0.96410\n",
      "[  14]  Training error: 0.01273 and Training accuracy: 0.98727 Test error: 0.03350 and Test accuracy: 0.96650\n",
      "[  15]  Training error: 0.01132 and Training accuracy: 0.98868 Test error: 0.03100 and Test accuracy: 0.96900\n",
      "[  16]  Training error: 0.01547 and Training accuracy: 0.98453 Test error: 0.03460 and Test accuracy: 0.96540\n",
      "[  17]  Training error: 0.01587 and Training accuracy: 0.98413 Test error: 0.03450 and Test accuracy: 0.96550\n",
      "[  18]  Training error: 0.01350 and Training accuracy: 0.98650 Test error: 0.03200 and Test accuracy: 0.96800\n",
      "[  19]  Training error: 0.00985 and Training accuracy: 0.99015 Test error: 0.03110 and Test accuracy: 0.96890\n",
      "[  20]  Training error: 0.01173 and Training accuracy: 0.98827 Test error: 0.03310 and Test accuracy: 0.96690\n",
      "[  21]  Training error: 0.01300 and Training accuracy: 0.98700 Test error: 0.03240 and Test accuracy: 0.96760\n",
      "[  22]  Training error: 0.00817 and Training accuracy: 0.99183 Test error: 0.02950 and Test accuracy: 0.97050\n",
      "[  23]  Training error: 0.00818 and Training accuracy: 0.99182 Test error: 0.02890 and Test accuracy: 0.97110\n",
      "[  24]  Training error: 0.00908 and Training accuracy: 0.99092 Test error: 0.02900 and Test accuracy: 0.97100\n",
      "[  25]  Training error: 0.00713 and Training accuracy: 0.99287 Test error: 0.02810 and Test accuracy: 0.97190\n",
      "[  26]  Training error: 0.00628 and Training accuracy: 0.99372 Test error: 0.02990 and Test accuracy: 0.97010\n",
      "[  27]  Training error: 0.00997 and Training accuracy: 0.99003 Test error: 0.03330 and Test accuracy: 0.96670\n",
      "[  28]  Training error: 0.00538 and Training accuracy: 0.99462 Test error: 0.02730 and Test accuracy: 0.97270\n",
      "[  29]  Training error: 0.00622 and Training accuracy: 0.99378 Test error: 0.02910 and Test accuracy: 0.97090\n",
      "[  30]  Training error: 0.00593 and Training accuracy: 0.99407 Test error: 0.02740 and Test accuracy: 0.97260\n",
      "[  31]  Training error: 0.01333 and Training accuracy: 0.98667 Test error: 0.03480 and Test accuracy: 0.96520\n",
      "[  32]  Training error: 0.00450 and Training accuracy: 0.99550 Test error: 0.02700 and Test accuracy: 0.97300\n",
      "[  33]  Training error: 0.00343 and Training accuracy: 0.99657 Test error: 0.02800 and Test accuracy: 0.97200\n",
      "[  34]  Training error: 0.00510 and Training accuracy: 0.99490 Test error: 0.02930 and Test accuracy: 0.97070\n",
      "[  35]  Training error: 0.00175 and Training accuracy: 0.99825 Test error: 0.02650 and Test accuracy: 0.97350\n",
      "[  36]  Training error: 0.00237 and Training accuracy: 0.99763 Test error: 0.02690 and Test accuracy: 0.97310\n",
      "[  37]  Training error: 0.00187 and Training accuracy: 0.99813 Test error: 0.02610 and Test accuracy: 0.97390\n",
      "[  38]  Training error: 0.00080 and Training accuracy: 0.99920 Test error: 0.02610 and Test accuracy: 0.97390\n",
      "[  39]  Training error: 0.00038 and Training accuracy: 0.99962 Test error: 0.02540 and Test accuracy: 0.97460\n",
      "[  40]  Training error: 0.00028 and Training accuracy: 0.99972 Test error: 0.02410 and Test accuracy: 0.97590\n",
      "[  41]  Training error: 0.00015 and Training accuracy: 0.99985 Test error: 0.02450 and Test accuracy: 0.97550\n",
      "[  42]  Training error: 0.00013 and Training accuracy: 0.99987 Test error: 0.02460 and Test accuracy: 0.97540\n",
      "[  43]  Training error: 0.00010 and Training accuracy: 0.99990 Test error: 0.02500 and Test accuracy: 0.97500\n",
      "[  44]  Training error: 0.00008 and Training accuracy: 0.99992 Test error: 0.02510 and Test accuracy: 0.97490\n",
      "[  45]  Training error: 0.00007 and Training accuracy: 0.99993 Test error: 0.02530 and Test accuracy: 0.97470\n",
      "[  46]  Training error: 0.00003 and Training accuracy: 0.99997 Test error: 0.02550 and Test accuracy: 0.97450\n",
      "[  47]  Training error: 0.00002 and Training accuracy: 0.99998 Test error: 0.02530 and Test accuracy: 0.97470\n",
      "[  48]  Training error: 0.00002 and Training accuracy: 0.99998 Test error: 0.02530 and Test accuracy: 0.97470\n",
      "[  49]  Training error: 0.00002 and Training accuracy: 0.99998 Test error: 0.02530 and Test accuracy: 0.97470\n",
      "[  50]  Training error: 0.00002 and Training accuracy: 0.99998 Test error: 0.02550 and Test accuracy: 0.97450\n",
      "[  51]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02540 and Test accuracy: 0.97460\n",
      "[  52]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02580 and Test accuracy: 0.97420\n",
      "[  53]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02600 and Test accuracy: 0.97400\n",
      "[  54]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02610 and Test accuracy: 0.97390\n",
      "[  55]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02590 and Test accuracy: 0.97410\n",
      "[  56]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02590 and Test accuracy: 0.97410\n",
      "[  57]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02590 and Test accuracy: 0.97410\n",
      "[  58]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02580 and Test accuracy: 0.97420\n",
      "[  59]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02570 and Test accuracy: 0.97430\n",
      "[  60]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02590 and Test accuracy: 0.97410\n",
      "[  61]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02600 and Test accuracy: 0.97400\n",
      "[  62]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02600 and Test accuracy: 0.97400\n",
      "[  63]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02600 and Test accuracy: 0.97400\n",
      "[  64]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02590 and Test accuracy: 0.97410\n",
      "[  65]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02590 and Test accuracy: 0.97410\n",
      "[  66]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02590 and Test accuracy: 0.97410\n",
      "[  67]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02590 and Test accuracy: 0.97410\n",
      "[  68]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02590 and Test accuracy: 0.97410\n",
      "[  69]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02590 and Test accuracy: 0.97410\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100;\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True)\n",
    "\n",
    "print(\"Done:)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1\n",
    "\n",
    "1.a The principle of backpropagation is that after doing the forward pass and getting a prediction, the model computes how wrong it was using a loss function. The model then goes backward through the network and computes the gradients of this loss for every weight in the model. This is done by using the chain rule to circumvant the problem of calculating a differential. These gradients tell the algorithm how each weight influenced the error and how much. Gradient descent then updates all weights by small amounts in the direction that reduces the loss. This process is repeated layer by layer from the output back to the input.\n",
    "\n",
    "1.b Softmax is an activation function used at the output of a classifier. It takes all the scores of the last layer and converts them into probabilities that sum to 1. This lets the network express how likely each class is and enables easy predictions. Softmax is usually used with the cross entropy loss which measures how far the predicted probability distribution is from the correct class.\n",
    "\n",
    "1.c Several different output activation functions exist depending on the task. Sigmoid is used for binary classification. Softmax is used for multi-class classification. Linear output is used for regression. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The classification accuracy on test is 97.4\\%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Evaluating with eta=0.005\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.70335 and Training accuracy: 0.29665 Test error: 0.70090 and Test accuracy: 0.29910\n",
      "[   1]  Training error: 0.64730 and Training accuracy: 0.35270 Test error: 0.64320 and Test accuracy: 0.35680\n",
      "[   2]  Training error: 0.59963 and Training accuracy: 0.40037 Test error: 0.59790 and Test accuracy: 0.40210\n",
      "[   3]  Training error: 0.46410 and Training accuracy: 0.53590 Test error: 0.47480 and Test accuracy: 0.52520\n",
      "[   4]  Training error: 0.20897 and Training accuracy: 0.79103 Test error: 0.19900 and Test accuracy: 0.80100\n",
      "[   5]  Training error: 0.11450 and Training accuracy: 0.88550 Test error: 0.11060 and Test accuracy: 0.88940\n",
      "[   6]  Training error: 0.09195 and Training accuracy: 0.90805 Test error: 0.08760 and Test accuracy: 0.91240\n",
      "[   7]  Training error: 0.07567 and Training accuracy: 0.92433 Test error: 0.07400 and Test accuracy: 0.92600\n",
      "[   8]  Training error: 0.06367 and Training accuracy: 0.93633 Test error: 0.06370 and Test accuracy: 0.93630\n",
      "[   9]  Training error: 0.05412 and Training accuracy: 0.94588 Test error: 0.05370 and Test accuracy: 0.94630\n",
      "[  10]  Training error: 0.04687 and Training accuracy: 0.95313 Test error: 0.04890 and Test accuracy: 0.95110\n",
      "[  11]  Training error: 0.04092 and Training accuracy: 0.95908 Test error: 0.04400 and Test accuracy: 0.95600\n",
      "[  12]  Training error: 0.03688 and Training accuracy: 0.96312 Test error: 0.04070 and Test accuracy: 0.95930\n",
      "[  13]  Training error: 0.03353 and Training accuracy: 0.96647 Test error: 0.03850 and Test accuracy: 0.96150\n",
      "[  14]  Training error: 0.03050 and Training accuracy: 0.96950 Test error: 0.03600 and Test accuracy: 0.96400\n",
      "[  15]  Training error: 0.02832 and Training accuracy: 0.97168 Test error: 0.03390 and Test accuracy: 0.96610\n",
      "[  16]  Training error: 0.02625 and Training accuracy: 0.97375 Test error: 0.03260 and Test accuracy: 0.96740\n",
      "[  17]  Training error: 0.02470 and Training accuracy: 0.97530 Test error: 0.03140 and Test accuracy: 0.96860\n",
      "[  18]  Training error: 0.02292 and Training accuracy: 0.97708 Test error: 0.03110 and Test accuracy: 0.96890\n",
      "[  19]  Training error: 0.02150 and Training accuracy: 0.97850 Test error: 0.03040 and Test accuracy: 0.96960\n",
      "[  20]  Training error: 0.02033 and Training accuracy: 0.97967 Test error: 0.02990 and Test accuracy: 0.97010\n",
      "[  21]  Training error: 0.01937 and Training accuracy: 0.98063 Test error: 0.02940 and Test accuracy: 0.97060\n",
      "[  22]  Training error: 0.01830 and Training accuracy: 0.98170 Test error: 0.02940 and Test accuracy: 0.97060\n",
      "[  23]  Training error: 0.01730 and Training accuracy: 0.98270 Test error: 0.02890 and Test accuracy: 0.97110\n",
      "[  24]  Training error: 0.01575 and Training accuracy: 0.98425 Test error: 0.02710 and Test accuracy: 0.97290\n",
      "[  25]  Training error: 0.01443 and Training accuracy: 0.98557 Test error: 0.02670 and Test accuracy: 0.97330\n",
      "[  26]  Training error: 0.01343 and Training accuracy: 0.98657 Test error: 0.02640 and Test accuracy: 0.97360\n",
      "[  27]  Training error: 0.01237 and Training accuracy: 0.98763 Test error: 0.02600 and Test accuracy: 0.97400\n",
      "[  28]  Training error: 0.01148 and Training accuracy: 0.98852 Test error: 0.02520 and Test accuracy: 0.97480\n",
      "[  29]  Training error: 0.01072 and Training accuracy: 0.98928 Test error: 0.02500 and Test accuracy: 0.97500\n",
      "[  30]  Training error: 0.00998 and Training accuracy: 0.99002 Test error: 0.02480 and Test accuracy: 0.97520\n",
      "[  31]  Training error: 0.00933 and Training accuracy: 0.99067 Test error: 0.02450 and Test accuracy: 0.97550\n",
      "[  32]  Training error: 0.00895 and Training accuracy: 0.99105 Test error: 0.02420 and Test accuracy: 0.97580\n",
      "[  33]  Training error: 0.00842 and Training accuracy: 0.99158 Test error: 0.02350 and Test accuracy: 0.97650\n",
      "[  34]  Training error: 0.00790 and Training accuracy: 0.99210 Test error: 0.02340 and Test accuracy: 0.97660\n",
      "[  35]  Training error: 0.00747 and Training accuracy: 0.99253 Test error: 0.02330 and Test accuracy: 0.97670\n",
      "[  36]  Training error: 0.00700 and Training accuracy: 0.99300 Test error: 0.02310 and Test accuracy: 0.97690\n",
      "[  37]  Training error: 0.00642 and Training accuracy: 0.99358 Test error: 0.02330 and Test accuracy: 0.97670\n",
      "[  38]  Training error: 0.00590 and Training accuracy: 0.99410 Test error: 0.02310 and Test accuracy: 0.97690\n",
      "[  39]  Training error: 0.00547 and Training accuracy: 0.99453 Test error: 0.02310 and Test accuracy: 0.97690\n",
      "[  40]  Training error: 0.00523 and Training accuracy: 0.99477 Test error: 0.02330 and Test accuracy: 0.97670\n",
      "[  41]  Training error: 0.00498 and Training accuracy: 0.99502 Test error: 0.02360 and Test accuracy: 0.97640\n",
      "[  42]  Training error: 0.00473 and Training accuracy: 0.99527 Test error: 0.02340 and Test accuracy: 0.97660\n",
      "[  43]  Training error: 0.00432 and Training accuracy: 0.99568 Test error: 0.02330 and Test accuracy: 0.97670\n",
      "[  44]  Training error: 0.00402 and Training accuracy: 0.99598 Test error: 0.02370 and Test accuracy: 0.97630\n",
      "[  45]  Training error: 0.00385 and Training accuracy: 0.99615 Test error: 0.02340 and Test accuracy: 0.97660\n",
      "[  46]  Training error: 0.00365 and Training accuracy: 0.99635 Test error: 0.02340 and Test accuracy: 0.97660\n",
      "[  47]  Training error: 0.00358 and Training accuracy: 0.99642 Test error: 0.02350 and Test accuracy: 0.97650\n",
      "[  48]  Training error: 0.00365 and Training accuracy: 0.99635 Test error: 0.02370 and Test accuracy: 0.97630\n",
      "[  49]  Training error: 0.00358 and Training accuracy: 0.99642 Test error: 0.02400 and Test accuracy: 0.97600\n",
      "[  50]  Training error: 0.00357 and Training accuracy: 0.99643 Test error: 0.02410 and Test accuracy: 0.97590\n",
      "[  51]  Training error: 0.00360 and Training accuracy: 0.99640 Test error: 0.02410 and Test accuracy: 0.97590\n",
      "[  52]  Training error: 0.00338 and Training accuracy: 0.99662 Test error: 0.02420 and Test accuracy: 0.97580\n",
      "[  53]  Training error: 0.00277 and Training accuracy: 0.99723 Test error: 0.02380 and Test accuracy: 0.97620\n",
      "[  54]  Training error: 0.00223 and Training accuracy: 0.99777 Test error: 0.02420 and Test accuracy: 0.97580\n",
      "[  55]  Training error: 0.00183 and Training accuracy: 0.99817 Test error: 0.02400 and Test accuracy: 0.97600\n",
      "[  56]  Training error: 0.00157 and Training accuracy: 0.99843 Test error: 0.02390 and Test accuracy: 0.97610\n",
      "[  57]  Training error: 0.00132 and Training accuracy: 0.99868 Test error: 0.02360 and Test accuracy: 0.97640\n",
      "[  58]  Training error: 0.00113 and Training accuracy: 0.99887 Test error: 0.02370 and Test accuracy: 0.97630\n",
      "[  59]  Training error: 0.00105 and Training accuracy: 0.99895 Test error: 0.02350 and Test accuracy: 0.97650\n",
      "[  60]  Training error: 0.00095 and Training accuracy: 0.99905 Test error: 0.02330 and Test accuracy: 0.97670\n",
      "[  61]  Training error: 0.00080 and Training accuracy: 0.99920 Test error: 0.02310 and Test accuracy: 0.97690\n",
      "[  62]  Training error: 0.00068 and Training accuracy: 0.99932 Test error: 0.02280 and Test accuracy: 0.97720\n",
      "[  63]  Training error: 0.00063 and Training accuracy: 0.99937 Test error: 0.02280 and Test accuracy: 0.97720\n",
      "[  64]  Training error: 0.00055 and Training accuracy: 0.99945 Test error: 0.02290 and Test accuracy: 0.97710\n",
      "[  65]  Training error: 0.00050 and Training accuracy: 0.99950 Test error: 0.02310 and Test accuracy: 0.97690\n",
      "[  66]  Training error: 0.00045 and Training accuracy: 0.99955 Test error: 0.02280 and Test accuracy: 0.97720\n",
      "[  67]  Training error: 0.00040 and Training accuracy: 0.99960 Test error: 0.02290 and Test accuracy: 0.97710\n",
      "[  68]  Training error: 0.00038 and Training accuracy: 0.99962 Test error: 0.02290 and Test accuracy: 0.97710\n",
      "[  69]  Training error: 0.00033 and Training accuracy: 0.99967 Test error: 0.02260 and Test accuracy: 0.97740\n",
      "Done:)\n",
      "\n",
      "Evaluating with eta=0.5\n",
      "Training for 70 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6973/3670988153.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0]  Training error: 0.88763 and Training accuracy: 0.11237 Test error: 0.88650 and Test accuracy: 0.11350\n",
      "[   1]  Training error: 0.90070 and Training accuracy: 0.09930 Test error: 0.89680 and Test accuracy: 0.10320\n",
      "[   2]  Training error: 0.88763 and Training accuracy: 0.11237 Test error: 0.88650 and Test accuracy: 0.11350\n",
      "[   3]  Training error: 0.90248 and Training accuracy: 0.09752 Test error: 0.90260 and Test accuracy: 0.09740\n",
      "[   4]  Training error: 0.90128 and Training accuracy: 0.09872 Test error: 0.90200 and Test accuracy: 0.09800\n",
      "[   5]  Training error: 0.88763 and Training accuracy: 0.11237 Test error: 0.88650 and Test accuracy: 0.11350\n",
      "[   6]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[   7]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[   8]  Training error: 0.90085 and Training accuracy: 0.09915 Test error: 0.89910 and Test accuracy: 0.10090\n",
      "[   9]  Training error: 0.90085 and Training accuracy: 0.09915 Test error: 0.89910 and Test accuracy: 0.10090\n",
      "[  10]  Training error: 0.90263 and Training accuracy: 0.09737 Test error: 0.90180 and Test accuracy: 0.09820\n",
      "[  11]  Training error: 0.90263 and Training accuracy: 0.09737 Test error: 0.90180 and Test accuracy: 0.09820\n",
      "[  12]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  13]  Training error: 0.90128 and Training accuracy: 0.09872 Test error: 0.90200 and Test accuracy: 0.09800\n",
      "[  14]  Training error: 0.88763 and Training accuracy: 0.11237 Test error: 0.88650 and Test accuracy: 0.11350\n",
      "[  15]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  16]  Training error: 0.88763 and Training accuracy: 0.11237 Test error: 0.88650 and Test accuracy: 0.11350\n",
      "[  17]  Training error: 0.89782 and Training accuracy: 0.10218 Test error: 0.89900 and Test accuracy: 0.10100\n",
      "[  18]  Training error: 0.90263 and Training accuracy: 0.09737 Test error: 0.90180 and Test accuracy: 0.09820\n",
      "[  19]  Training error: 0.89782 and Training accuracy: 0.10218 Test error: 0.89900 and Test accuracy: 0.10100\n",
      "[  20]  Training error: 0.88763 and Training accuracy: 0.11237 Test error: 0.88650 and Test accuracy: 0.11350\n",
      "[  21]  Training error: 0.90248 and Training accuracy: 0.09752 Test error: 0.90260 and Test accuracy: 0.09740\n",
      "[  22]  Training error: 0.90965 and Training accuracy: 0.09035 Test error: 0.91080 and Test accuracy: 0.08920\n",
      "[  23]  Training error: 0.88763 and Training accuracy: 0.11237 Test error: 0.88650 and Test accuracy: 0.11350\n",
      "[  24]  Training error: 0.89782 and Training accuracy: 0.10218 Test error: 0.89900 and Test accuracy: 0.10100\n",
      "[  25]  Training error: 0.89782 and Training accuracy: 0.10218 Test error: 0.89900 and Test accuracy: 0.10100\n",
      "[  26]  Training error: 0.90070 and Training accuracy: 0.09930 Test error: 0.89680 and Test accuracy: 0.10320\n",
      "[  27]  Training error: 0.90137 and Training accuracy: 0.09863 Test error: 0.90420 and Test accuracy: 0.09580\n",
      "[  28]  Training error: 0.88763 and Training accuracy: 0.11237 Test error: 0.88650 and Test accuracy: 0.11350\n",
      "[  29]  Training error: 0.90263 and Training accuracy: 0.09737 Test error: 0.90180 and Test accuracy: 0.09820\n",
      "[  30]  Training error: 0.90137 and Training accuracy: 0.09863 Test error: 0.90420 and Test accuracy: 0.09580\n",
      "[  31]  Training error: 0.89782 and Training accuracy: 0.10218 Test error: 0.89900 and Test accuracy: 0.10100\n",
      "[  32]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  33]  Training error: 0.90128 and Training accuracy: 0.09872 Test error: 0.90200 and Test accuracy: 0.09800\n",
      "[  34]  Training error: 0.90965 and Training accuracy: 0.09035 Test error: 0.91080 and Test accuracy: 0.08920\n",
      "[  35]  Training error: 0.90085 and Training accuracy: 0.09915 Test error: 0.89910 and Test accuracy: 0.10090\n",
      "[  36]  Training error: 0.90137 and Training accuracy: 0.09863 Test error: 0.90420 and Test accuracy: 0.09580\n",
      "[  37]  Training error: 0.90085 and Training accuracy: 0.09915 Test error: 0.89910 and Test accuracy: 0.10090\n",
      "[  38]  Training error: 0.89782 and Training accuracy: 0.10218 Test error: 0.89900 and Test accuracy: 0.10100\n",
      "[  39]  Training error: 0.90137 and Training accuracy: 0.09863 Test error: 0.90420 and Test accuracy: 0.09580\n",
      "[  40]  Training error: 0.90248 and Training accuracy: 0.09752 Test error: 0.90260 and Test accuracy: 0.09740\n",
      "[  41]  Training error: 0.90248 and Training accuracy: 0.09752 Test error: 0.90260 and Test accuracy: 0.09740\n",
      "[  42]  Training error: 0.90248 and Training accuracy: 0.09752 Test error: 0.90260 and Test accuracy: 0.09740\n",
      "[  43]  Training error: 0.90965 and Training accuracy: 0.09035 Test error: 0.91080 and Test accuracy: 0.08920\n",
      "[  44]  Training error: 0.90137 and Training accuracy: 0.09863 Test error: 0.90420 and Test accuracy: 0.09580\n",
      "[  45]  Training error: 0.89782 and Training accuracy: 0.10218 Test error: 0.89900 and Test accuracy: 0.10100\n",
      "[  46]  Training error: 0.90263 and Training accuracy: 0.09737 Test error: 0.90180 and Test accuracy: 0.09820\n",
      "[  47]  Training error: 0.90137 and Training accuracy: 0.09863 Test error: 0.90420 and Test accuracy: 0.09580\n",
      "[  48]  Training error: 0.90248 and Training accuracy: 0.09752 Test error: 0.90260 and Test accuracy: 0.09740\n",
      "[  49]  Training error: 0.90965 and Training accuracy: 0.09035 Test error: 0.91080 and Test accuracy: 0.08920\n",
      "[  50]  Training error: 0.88763 and Training accuracy: 0.11237 Test error: 0.88650 and Test accuracy: 0.11350\n",
      "[  51]  Training error: 0.90263 and Training accuracy: 0.09737 Test error: 0.90180 and Test accuracy: 0.09820\n",
      "[  52]  Training error: 0.90965 and Training accuracy: 0.09035 Test error: 0.91080 and Test accuracy: 0.08920\n",
      "[  53]  Training error: 0.90070 and Training accuracy: 0.09930 Test error: 0.89680 and Test accuracy: 0.10320\n",
      "[  54]  Training error: 0.90263 and Training accuracy: 0.09737 Test error: 0.90180 and Test accuracy: 0.09820\n",
      "[  55]  Training error: 0.89782 and Training accuracy: 0.10218 Test error: 0.89900 and Test accuracy: 0.10100\n",
      "[  56]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  57]  Training error: 0.90128 and Training accuracy: 0.09872 Test error: 0.90200 and Test accuracy: 0.09800\n",
      "[  58]  Training error: 0.90965 and Training accuracy: 0.09035 Test error: 0.91080 and Test accuracy: 0.08920\n",
      "[  59]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  60]  Training error: 0.90248 and Training accuracy: 0.09752 Test error: 0.90260 and Test accuracy: 0.09740\n",
      "[  61]  Training error: 0.90128 and Training accuracy: 0.09872 Test error: 0.90200 and Test accuracy: 0.09800\n",
      "[  62]  Training error: 0.90085 and Training accuracy: 0.09915 Test error: 0.89910 and Test accuracy: 0.10090\n",
      "[  63]  Training error: 0.90965 and Training accuracy: 0.09035 Test error: 0.91080 and Test accuracy: 0.08920\n",
      "[  64]  Training error: 0.90263 and Training accuracy: 0.09737 Test error: 0.90180 and Test accuracy: 0.09820\n",
      "[  65]  Training error: 0.89782 and Training accuracy: 0.10218 Test error: 0.89900 and Test accuracy: 0.10100\n",
      "[  66]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  67]  Training error: 0.90248 and Training accuracy: 0.09752 Test error: 0.90260 and Test accuracy: 0.09740\n",
      "[  68]  Training error: 0.88763 and Training accuracy: 0.11237 Test error: 0.88650 and Test accuracy: 0.11350\n",
      "[  69]  Training error: 0.90128 and Training accuracy: 0.09872 Test error: 0.90200 and Test accuracy: 0.09800\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100;\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "etas = [0.005, 0.5]\n",
    "for eta in etas:\n",
    "    print(f\"Evaluating with eta={eta}\")\n",
    "    mlp.evaluate(train_data, train_labels, valid_data, valid_labels,eta=eta,\n",
    "                 eval_train=True)\n",
    "\n",
    "    print(\"Done:)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. With $eta = 0.005$ the learning does happen but it takes more epochs to do so. The errors also don't fluctuate as much as with a higher eta.\n",
    "With $eta = 0.5$ the learning never happens so the error remains very high and the accuracy is very low (close to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functionality of a single hidden layer\n",
    "\n",
    "def f_Relu(X, deriv=False):\n",
    "    if deriv:\n",
    "        return (X > 0).astype(float)\n",
    "    return np.maximum(0, X)\n",
    "\n",
    "    \n",
    "class Layer_ReLu:\n",
    "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
    "                 activation=f_Relu):\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # Z is the matrix that holds output values\n",
    "        self.Z = np.zeros((batch_size, size[0]))\n",
    "        # The activation function is an externally defined function (with a\n",
    "        # derivative) that is stored here\n",
    "        self.activation = activation\n",
    "\n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        # S is the matrix that holds the inputs to this layer\n",
    "        self.S = None\n",
    "        # D is the matrix that holds the deltas for this layer\n",
    "        self.D = None\n",
    "        # Fp is the matrix that holds the derivatives of the activation function\n",
    "        self.Fp = None\n",
    "\n",
    "        if not is_input:\n",
    "            self.S = np.zeros((batch_size, size[0]))\n",
    "            self.D = np.zeros((batch_size, size[0]))\n",
    "\n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "\n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], batch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron_ReLu:\n",
    "    def __init__(self, layer_config, batch_size=100):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = batch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer_ReLu([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                self.layers.append(Layer_ReLu([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         activation=f_Relu))\n",
    "\n",
    "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
    "        self.layers.append(Layer_ReLu([layer_config[-1], None],\n",
    "                                 batch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=f_softmax))\n",
    "        print (\"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        \n",
    "        #exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
    "        #This line computes the error at the output layer by subtracting the true labels from the predicted outputs.\n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "            \n",
    "            #exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
    "            #This loop iterates backward through the hidden layers of the network, calculating the delta values \n",
    "            # for each layer based on the deltas of the next layer and the weights connecting them.\n",
    "            \n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=70, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "            \n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                \n",
    "                #exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
    "                # This line updates the weights of the network using the computed deltas and the learning rate (eta). \n",
    "                # Eta is a hyperparameter that controls how much to change the weights during each update.\n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                accuracy_train = 0.0\n",
    "        \n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    accuracy_train += np.sum(yhat == np.argmax(b_labels, axis=1))\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Training error: {1:.5f} and Training accuracy: {2:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train,\n",
    "                                                           accuracy_train/N_train))\n",
    "                \n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                accuracy_test = 0.0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    accuracy_test += np.sum(yhat == np.argmax(b_labels, axis=1))\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Test error: {1:.5f} and Test accuracy: {2:.5f}\").format(out_str,\n",
    "                                                       float(errs)/N_test,\n",
    "                                                       accuracy_test/N_test)\n",
    "\n",
    "            print (out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[   1]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[   2]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[   3]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[   4]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[   5]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[   6]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[   7]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[   8]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[   9]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  10]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  11]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  12]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  13]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  14]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  15]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  16]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  17]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  18]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  19]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  20]  Training error: 0.89558 and Training accuracy: 0.10442 Test error: 0.89720 and Test accuracy: 0.10280\n",
      "[  21]  Training error: 0.53505 and Training accuracy: 0.46495 Test error: 0.53970 and Test accuracy: 0.46030\n",
      "[  22]  Training error: 0.08087 and Training accuracy: 0.91913 Test error: 0.07880 and Test accuracy: 0.92120\n",
      "[  23]  Training error: 0.04408 and Training accuracy: 0.95592 Test error: 0.04660 and Test accuracy: 0.95340\n",
      "[  24]  Training error: 0.03720 and Training accuracy: 0.96280 Test error: 0.04240 and Test accuracy: 0.95760\n",
      "[  25]  Training error: 0.02820 and Training accuracy: 0.97180 Test error: 0.03680 and Test accuracy: 0.96320\n",
      "[  26]  Training error: 0.02182 and Training accuracy: 0.97818 Test error: 0.03120 and Test accuracy: 0.96880\n",
      "[  27]  Training error: 0.02243 and Training accuracy: 0.97757 Test error: 0.03500 and Test accuracy: 0.96500\n",
      "[  28]  Training error: 0.01568 and Training accuracy: 0.98432 Test error: 0.02780 and Test accuracy: 0.97220\n",
      "[  29]  Training error: 0.01955 and Training accuracy: 0.98045 Test error: 0.03340 and Test accuracy: 0.96660\n",
      "[  30]  Training error: 0.02125 and Training accuracy: 0.97875 Test error: 0.03520 and Test accuracy: 0.96480\n",
      "[  31]  Training error: 0.01382 and Training accuracy: 0.98618 Test error: 0.02950 and Test accuracy: 0.97050\n",
      "[  32]  Training error: 0.01368 and Training accuracy: 0.98632 Test error: 0.02780 and Test accuracy: 0.97220\n",
      "[  33]  Training error: 0.01448 and Training accuracy: 0.98552 Test error: 0.03120 and Test accuracy: 0.96880\n",
      "[  34]  Training error: 0.01293 and Training accuracy: 0.98707 Test error: 0.02880 and Test accuracy: 0.97120\n",
      "[  35]  Training error: 0.01193 and Training accuracy: 0.98807 Test error: 0.02990 and Test accuracy: 0.97010\n",
      "[  36]  Training error: 0.01130 and Training accuracy: 0.98870 Test error: 0.02940 and Test accuracy: 0.97060\n",
      "[  37]  Training error: 0.00722 and Training accuracy: 0.99278 Test error: 0.02800 and Test accuracy: 0.97200\n",
      "[  38]  Training error: 0.00902 and Training accuracy: 0.99098 Test error: 0.02920 and Test accuracy: 0.97080\n",
      "[  39]  Training error: 0.01095 and Training accuracy: 0.98905 Test error: 0.03030 and Test accuracy: 0.96970\n",
      "[  40]  Training error: 0.00815 and Training accuracy: 0.99185 Test error: 0.02950 and Test accuracy: 0.97050\n",
      "[  41]  Training error: 0.01558 and Training accuracy: 0.98442 Test error: 0.03250 and Test accuracy: 0.96750\n",
      "[  42]  Training error: 0.00917 and Training accuracy: 0.99083 Test error: 0.02970 and Test accuracy: 0.97030\n",
      "[  43]  Training error: 0.00658 and Training accuracy: 0.99342 Test error: 0.02730 and Test accuracy: 0.97270\n",
      "[  44]  Training error: 0.00437 and Training accuracy: 0.99563 Test error: 0.02550 and Test accuracy: 0.97450\n",
      "[  45]  Training error: 0.00643 and Training accuracy: 0.99357 Test error: 0.02890 and Test accuracy: 0.97110\n",
      "[  46]  Training error: 0.00452 and Training accuracy: 0.99548 Test error: 0.02710 and Test accuracy: 0.97290\n",
      "[  47]  Training error: 0.00658 and Training accuracy: 0.99342 Test error: 0.02710 and Test accuracy: 0.97290\n",
      "[  48]  Training error: 0.00463 and Training accuracy: 0.99537 Test error: 0.02620 and Test accuracy: 0.97380\n",
      "[  49]  Training error: 0.01222 and Training accuracy: 0.98778 Test error: 0.03200 and Test accuracy: 0.96800\n",
      "[  50]  Training error: 0.00975 and Training accuracy: 0.99025 Test error: 0.02750 and Test accuracy: 0.97250\n",
      "[  51]  Training error: 0.00480 and Training accuracy: 0.99520 Test error: 0.02710 and Test accuracy: 0.97290\n",
      "[  52]  Training error: 0.00190 and Training accuracy: 0.99810 Test error: 0.02190 and Test accuracy: 0.97810\n",
      "[  53]  Training error: 0.00220 and Training accuracy: 0.99780 Test error: 0.02230 and Test accuracy: 0.97770\n",
      "[  54]  Training error: 0.00113 and Training accuracy: 0.99887 Test error: 0.02160 and Test accuracy: 0.97840\n",
      "[  55]  Training error: 0.00458 and Training accuracy: 0.99542 Test error: 0.02390 and Test accuracy: 0.97610\n",
      "[  56]  Training error: 0.00300 and Training accuracy: 0.99700 Test error: 0.02410 and Test accuracy: 0.97590\n",
      "[  57]  Training error: 0.00323 and Training accuracy: 0.99677 Test error: 0.02510 and Test accuracy: 0.97490\n",
      "[  58]  Training error: 0.00580 and Training accuracy: 0.99420 Test error: 0.02710 and Test accuracy: 0.97290\n",
      "[  59]  Training error: 0.00155 and Training accuracy: 0.99845 Test error: 0.02400 and Test accuracy: 0.97600\n",
      "[  60]  Training error: 0.00037 and Training accuracy: 0.99963 Test error: 0.02230 and Test accuracy: 0.97770\n",
      "[  61]  Training error: 0.00033 and Training accuracy: 0.99967 Test error: 0.02250 and Test accuracy: 0.97750\n",
      "[  62]  Training error: 0.00022 and Training accuracy: 0.99978 Test error: 0.02220 and Test accuracy: 0.97780\n",
      "[  63]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02150 and Test accuracy: 0.97850\n",
      "[  64]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02140 and Test accuracy: 0.97860\n",
      "[  65]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02160 and Test accuracy: 0.97840\n",
      "[  66]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02170 and Test accuracy: 0.97830\n",
      "[  67]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02190 and Test accuracy: 0.97810\n",
      "[  68]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02170 and Test accuracy: 0.97830\n",
      "[  69]  Training error: 0.00000 and Training accuracy: 1.00000 Test error: 0.02150 and Test accuracy: 0.97850\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100;\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron_ReLu(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,eta=5e-3,\n",
    "             eval_train=True)\n",
    "\n",
    "print(\"Done:)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. With a learning rate of 0.05 no learning is done because the model can't get out of the initialisation of weights with negative and small numbers. With an eta of 0.005 the learning begins after 12 epochs and the results are similar to sigmoid."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appliedAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
