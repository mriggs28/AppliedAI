

# ‚úÖ **1. Local vs Distributed Representations**

### **Local (One-Hot) Representation**

* Each concept = a vector with one ‚Äú1‚Äù and the rest ‚Äú0‚Äù.
* Example:

  * Cat = [1, 0, 0, 0]
  * Dog = [0, 1, 0, 0]
* **Properties**:

  * Mutually orthogonal.
  * No notion of similarity.
  * Very inefficient in high dimension.

### **Distributed / High-Dimensional Representation**

* Each concept = a long dense vector (e.g., 10,000 dimensions) of random values like ¬±1.
* Similar concepts can share structure, enabling:

  * **Robustness**: random noise barely changes similarity.
  * **Compactness**: you can store many patterns via superposition.
  * **Compositionality**: vectors can be combined algebraically (binding, permutation, superposition).

---

# ‚úÖ **2. Random HD Vectors Are Nearly Orthogonal**

In a high-dimensional space (d ‚âà 10,000), two random ¬±1 vectors:

$
\mathbb{E}[\text{cosine}(x,y)] = 0,\quad \text{Var}\approx\frac{1}{\sqrt{d}}
$

So cosine similarity ‚âà 0 ‚Üí almost orthogonal ‚Üí nearly independent.

**Why useful?**

* Each vector behaves like a unique symbol.
* You can reliably separate many items.

---

# ‚úÖ **3. Core HDC/VSA Operations**

### **Superposition**

* Purpose: *store multiple items together*.
* Operation: elementwise addition
  $
  H = A + B + C
  $
* Later retrieve using cosine similarity:
  ( $ \text{sim}(A, H) \approx \text{high} $).

### **Binding**

* Purpose: *associate two items* (e.g., role‚Äìfiller pairs).
* Typical operation: elementwise multiplication
  $
  P = A \odot B
  $
* Properties:

  * Reversible: ( B = A \odot P )
  * Creates a vector dissimilar to A and B separately ‚Üí avoids interference.

### **Similarity**

* Measure to compare vectors: cosine similarity or dot-product.
* Works well because HD vectors have predictable distributions.

---

# ‚úÖ **4. Why Random ¬±1 Vectors Are Used**

* **Easy to generate**
* **Space-efficient**
* **Computationally cheap**: operations = adds & multiplies
* **Balanced**: equal number of +1 and ‚Äì1 ‚Üí zero-mean ‚Üí no bias
* **Independent dimensions** ‚Üí high capacity encoding

---

# ‚úÖ **5. Centroid-Based HD Classification**

Training-free classification:

1. For each class, sum all its vectors:
   $
   C_k = \sum_{i \in \text{class k}} x_i
   $
2. Normalize centroids.
3. For a test vector x, compute cosine similarity to each centroid.
4. Predict class with highest similarity.

**Advantages**:

* No gradient descent.
* Extremely fast.
* Very robust.

---

# ‚úÖ **6. n-grams vs Hypervectors**

### **n-grams**

* Represent sequences using one-hot vectors for words.
* Curse of dimensionality: number of possible n-grams blows up.

### **Hypervectors**

* Encode sequence structure via:

  * permutation (shifting)
  * binding
  * superposition
* Very compact and robust to noise.

---

# ‚úÖ **7. Benefits of HD Representations**

* **Compositionality**: combine symbols algebraically.
* **Robustness**: noise affects only a few dimensions.
* **Scalability**: more dimensions = more capacity.
* **Symbolic reasoning**: supports role‚Äìfiller, structures, sequences.

---

# üß† **SELF-ORGANIZING MAPS (SOMs)**

# ‚úÖ **8. What is a Self-Organizing Map?**

SOM = unsupervised neural network that maps high-dimensional data ‚Üí 2D grid while preserving topology.

Meaning:

* Nearby neurons represent similar data.
* Far neurons represent dissimilar data.

---

# ‚úÖ **9. SOM Learning Algorithm**

For each sample:

1. **Compute BMU (Best Matching Unit)**
   Neuron whose weight vector is closest to input:
   $
   \text{BMU} = \arg\min_j |x - w_j|
   $

2. **Update BMU and neighbors**:
   $
   w_j(t+1) = w_j(t) + \eta(t),h_{j,\text{BMU}}(t),(x - w_j(t))
   $

3. **Decay**:

   * learning rate Œ∑(t)
   * neighborhood radius œÉ(t)

---

# ‚úÖ **10. Neighborhood Function**

Typically Gaussian:

$
h_{j,\text{BMU}}(t) =
\exp\left(-\frac{d(j,\text{BMU})^2}{2\sigma(t)^2}\right)
$

Purpose:

* Spread learning across neighbors.
* Smooth the map early.
* Refine it later as œÉ shrinks.

---

# ‚úÖ **11. Effects of SOM Hyperparameters**

* **Learning rate too high** ‚Üí oscillations; unstable map.
* **Learning rate too low** ‚Üí very slow convergence.
* **Radius decays too quickly** ‚Üí poor global organization; map fragments.
* **Radius too large for too long** ‚Üí everything becomes similar.

---

# ‚úÖ **12. Grid Resolution**

* Larger grid = more neurons ‚Üí higher map resolution.
* Reduces quantization error.
* Increases training time.

---

# ‚úÖ **13. Biological Inspiration**

SOM mimics cortical maps:

* **Competition**: neurons compete to respond.
* **Cooperation**: neighbors also update.
* **Lateral inhibition**: ‚Äúwinner inhibits neighbors‚Äù mechanism.

---

# ü§ñ **MLP + BACKPROPAGATION**

# ‚úÖ **14. MLP Structure**

* Input layer ‚Üí passes features.
* Hidden layers ‚Üí nonlinear transformations.
* Output layer ‚Üí prediction (e.g., Softmax).

Connections:
$
y = f(W_2 , f(W_1 x + b_1) + b_2)
$

---

# ‚úÖ **15. Backpropagation Overview**

Goal: minimize loss L.

Steps:

1. Forward pass ‚Üí compute predictions.
2. Compute loss L.
3. Use chain rule to compute gradients:
   $
   \frac{\partial L}{\partial W}
   $
4. Gradient descent update:
   $
   W \leftarrow W - \eta \frac{\partial L}{\partial W}
   $

---

# ‚úÖ **16. Activation Functions**

| Activation  | Range   | Advantages                 |
| ----------- | ------- | -------------------------- |
| **Sigmoid** | (0, 1)  | Probabilistic output       |
| **tanh**    | (-1, 1) | Zero-centered              |
| **ReLU**    | [0, ‚àû)  | Avoids vanishing gradients |

---

# ‚úÖ **17. Softmax**

Turns logits z into probabilities:

$
p_i = \frac{e^{z_i}}{\sum_j e^{z_j}}
$

Used for multiclass classification.

---

# ‚úÖ **18. Learning Rate Influence**

* **Too high** ‚Üí divergence, oscillations.
* **Too low** ‚Üí very slow learning or stuck in bad minima.

---

# ‚úÖ **19. Weight Initialization**

Goal: avoid zero gradients & symmetry.

* **Xavier**: for tanh/sigmoid
* **He**: for ReLU

Ensures variance does not explode or vanish through layers.

---

# ‚úÖ **20. Common Loss Functions**

* **Cross-entropy**: classification
* **MSE**: regression
* **Negative log-likelihood** = cross-entropy with Softmax

---

# ‚úÖ **21. ReLU vs Sigmoid**

ReLU:

* No saturation for positive inputs.
* Prevents vanishing gradients.
* Faster convergence.

Sigmoid:

* Smooth but saturates ‚Üí vanishing gradients.
* Used sometimes in output layers (binary classification).

---

# ‚úÖ **22. Overfitting & Early Stopping**

* Too many epochs ‚Üí model memorizes data.
* Early stopping monitors validation loss and stops when it increases.
* Prevents overfitting.

---

# ‚úÖ **23. Avoiding Vanishing/Exploding Gradients**

Methods:

* ReLU activations
* Xavier/He initialization
* Batch normalization
* Gradient clipping

---

# üéì **LEARNING PARADIGM ASSOCIATIONS**

| Model           | Paradigm      | Explanation                         |
| --------------- | ------------- | ----------------------------------- |
| **SOM**         | Unsupervised  | Learns topology from unlabeled data |
| **MLP**         | Supervised    | Needs labeled data for backprop     |
| **HD Centroid** | Training-free | Just accumulates vectors            |

---

# ‚úÖ **24. Conceptual Differences Between SOM and MLP**

* **SOM**:

  * Creates a map
  * Learns structure/topology
  * Unsupervised

* **MLP**:

  * Learns class boundaries
  * Requires labels
  * Optimizes a loss

---

# ‚úÖ **25. Confusion Matrix Interpretation**

Confusion matrix M‚Ççi,j‚Çé:

* Rows = true labels
* Columns = predicted labels
* Diagonal = correct preds
* Off-diagonal = errors

Useful for analyzing:

* Which classes are confused
* Precision/recall
* Accuracy

---

# üß† **26. Advantages of HD Representations**

* **Scalability**: add more items easily.
* **Noise tolerance**: small corruption doesn't change cosine similarity much.
* **Explainability**: operations (binding/superposition) are interpretable.

---

# ü§ù **27. How HD, SOM, and Backprop Complement Each Other**

| Method             | Paradigm      | Strength                 |
| ------------------ | ------------- | ------------------------ |
| **HD Computing**   | Training-free | Fast, robust, symbolic   |
| **SOM**            | Unsupervised  | Topological mapping      |
| **MLP + Backprop** | Supervised    | Learn complex boundaries |

They cover the three fundamental learning paradigms.

---

# üßÆ **28. Numeric Examples You Must Know**

### **Cosine similarity**

$
\cos(x,y) = \frac{x\cdot y}{|x| |y|}
$

Example:
x = [1, -1, 1], y = [1, 1, -1]
‚Üí dot = 1 - 1 - 1 = -1
‚Üí cosine ‚âà -1 / 3 = ‚Äì0.33

---

### **SOM BMU Update Example**

If:

* w = [1, 1]
* x = [3, 2]
* learning rate = 0.5
* h = 1 (BMU)

Update:

$
w \leftarrow w + 0.5 ([3,2]-[1,1]) = [1,1] + 0.5[2,1] = [2,1.5]
$

---

### **Backprop Weight Update Example**

Simple rule:

$
w \leftarrow w - \eta \frac{\partial L}{\partial w}
$

If:

* gradient = 0.2
* Œ∑ = 0.1

Then:
$
w \leftarrow w - 0.1 \times 0.2 = w - 0.02
$

